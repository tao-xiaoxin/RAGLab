{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_5KKQWNue_w"
   },
   "source": [
    "# Google Colab å®ç”¨æ•™ç¨‹\n",
    "\n",
    "ä¸“æ³¨äºæœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœ€å®ç”¨çš„åŠŸèƒ½ï¼šæ•°æ®é›†ç®¡ç†ã€æ¨¡å‹è®­ç»ƒä¸ä¿å­˜ã€‚\n",
    "\n",
    "## ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½\n",
    "1. [äº‘ç›˜æŒ‚è½½ä¸æ•°æ®é›†åŠ è½½](#1-äº‘ç›˜æŒ‚è½½ä¸æ•°æ®é›†åŠ è½½)\n",
    "2. [GPUé…ç½®ä¸éªŒè¯](#2-GPUé…ç½®ä¸éªŒè¯)  \n",
    "3. [æ¨¡å‹è®­ç»ƒä¸ä¿å­˜](#3-æ¨¡å‹è®­ç»ƒä¸ä¿å­˜)\n",
    "4. [å®ç”¨æŠ€å·§](#4-å®ç”¨æŠ€å·§)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. äº‘ç›˜æŒ‚è½½ä¸æ•°æ®é›†åŠ è½½\n",
    "\n",
    "### 1.1 æŒ‚è½½Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19376,
     "status": "ok",
     "timestamp": 1758550295431,
     "user": {
      "displayName": "Xiaoxin Tao",
      "userId": "17474859127382790787"
     },
     "user_tz": 420
    },
    "id": "H1j3NSzHrKqj",
    "outputId": "f5fda6bc-ad77-4843-b13c-a47a4d65c82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoOdA8C8xQmn"
   },
   "source": [
    "### 1.2 åŠ è½½æ•°æ®é›†\n",
    "\n",
    "ä»¥ä¸‹æ˜¯å¸¸è§çš„æ•°æ®é›†åŠ è½½æ–¹å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1758550409330,
     "user": {
      "displayName": "Xiaoxin Tao",
      "userId": "17474859127382790787"
     },
     "user_tz": 420
    },
    "id": "CVAJu01Dsl_E",
    "outputId": "f77fbad0-b232-4191-dcfc-ed62cabfe897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 22 14:13:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 1. ä»äº‘ç›˜åŠ è½½æ•°æ®é›†\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# è®¾ç½®æ•°æ®é›†è·¯å¾„ï¼ˆäº‘ç›˜ä¸­çš„è·¯å¾„ï¼‰\n",
    "dataset_path = \"/content/drive/MyDrive/datasets/\"\n",
    "\n",
    "# åŠ è½½CSVæ•°æ®é›†\n",
    "# df = pd.read_csv(dataset_path + \"train.csv\")\n",
    "# print(f\"æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "\n",
    "# 2. ä»äº‘ç›˜åŠ è½½å›¾åƒæ•°æ®é›†\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å›¾åƒæ•°æ®é›†è·¯å¾„ç¤ºä¾‹\n",
    "# image_folder = \"/content/drive/MyDrive/datasets/images/\"\n",
    "# image_files = os.listdir(image_folder)\n",
    "# print(f\"æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶\")\n",
    "\n",
    "# 3. ä»äº‘ç›˜åŠ è½½é¢„å¤„ç†å¥½çš„numpyæ•°ç»„\n",
    "# X_train = np.load(\"/content/drive/MyDrive/datasets/X_train.npy\")\n",
    "# y_train = np.load(\"/content/drive/MyDrive/datasets/y_train.npy\")\n",
    "# print(f\"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X_train.shape}, æ ‡ç­¾å½¢çŠ¶: {y_train.shape}\")\n",
    "\n",
    "print(\"æ•°æ®é›†åŠ è½½æ¨¡æ¿å·²å‡†å¤‡å¥½ï¼Œè¯·æ ¹æ®å®é™…è·¯å¾„ä¿®æ”¹ä»£ç \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPUé…ç½®ä¸éªŒè¯\n",
    "\n",
    "### 2.1 æŸ¥çœ‹åˆ†é…çš„GPU\n",
    "\n",
    "**é‡è¦æç¤º**: è¦ä½¿ç”¨GPUï¼Œè¯·å…ˆè®¾ç½®è¿è¡Œæ—¶ç±»å‹ï¼š\n",
    "- ç‚¹å‡» **è¿è¡Œæ—¶** â†’ **æ›´æ”¹è¿è¡Œæ—¶ç±»å‹** â†’ **ç¡¬ä»¶åŠ é€Ÿå™¨** â†’ é€‰æ‹© **GPU**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹åˆ†é…åˆ°çš„GPUè¯¦ç»†ä¿¡æ¯\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('âŒ æœªè¿æ¥åˆ°GPU')\n",
    "  print('è¯·æ£€æŸ¥è¿è¡Œæ—¶è®¾ç½®: è¿è¡Œæ—¶ -> æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ -> GPU')\n",
    "else:\n",
    "  print('âœ… GPUä¿¡æ¯:')\n",
    "  print(gpu_info)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# æ£€æŸ¥GPUå’Œæ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=== æ·±åº¦å­¦ä¹ æ¡†æ¶ GPU æ”¯æŒæ£€æŸ¥ ===\")\n",
    "\n",
    "# PyTorch GPUæ£€æŸ¥\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"å½“å‰GPUè®¾å¤‡: cuda:{torch.cuda.current_device()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# TensorFlow GPUæ£€æŸ¥  \n",
    "print(f\"TensorFlowç‰ˆæœ¬: {tf.__version__}\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"TensorFlowæ£€æµ‹åˆ°çš„GPU: {len(gpus)} ä¸ª\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "# è®¾ç½®GPUå†…å­˜å¢é•¿ï¼ˆé¿å…å ç”¨å…¨éƒ¨æ˜¾å­˜ï¼‰\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"âœ… GPUå†…å­˜å¢é•¿æ¨¡å¼å·²è®¾ç½®\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âŒ è®¾ç½®GPUå†…å­˜å¢é•¿å¤±è´¥: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹è®­ç»ƒä¸ä¿å­˜\n",
    "\n",
    "### 3.1 æ¨¡å‹ä¿å­˜åˆ°äº‘ç›˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ä¿å­˜ç¤ºä¾‹\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•\n",
    "model_save_path = \"/content/drive/MyDrive/models/\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# 1. PyTorchæ¨¡å‹ä¿å­˜\n",
    "\"\"\"\n",
    "# è®­ç»ƒå®Œæˆåä¿å­˜æ¨¡å‹\n",
    "model_name = f\"my_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth\"\n",
    "torch.save(model.state_dict(), model_save_path + model_name)\n",
    "print(f\"PyTorchæ¨¡å‹å·²ä¿å­˜: {model_save_path + model_name}\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŒ…æ‹¬æ¶æ„ï¼‰\n",
    "torch.save(model, model_save_path + f\"complete_model_{datetime.now().strftime('%Y%m%d_%H%M')}.pth\")\n",
    "\"\"\"\n",
    "\n",
    "# 2. TensorFlow/Kerasæ¨¡å‹ä¿å­˜\n",
    "\"\"\"\n",
    "# ä¿å­˜æ•´ä¸ªæ¨¡å‹\n",
    "model.save(model_save_path + f\"tf_model_{datetime.now().strftime('%Y%m%d_%H%M')}.h5\")\n",
    "\n",
    "# ä¿å­˜æƒé‡\n",
    "model.save_weights(model_save_path + f\"tf_weights_{datetime.now().strftime('%Y%m%d_%H%M')}.h5\")\n",
    "\n",
    "# ä¿å­˜ä¸ºSavedModelæ ¼å¼ï¼ˆæ¨èï¼‰\n",
    "model.save(model_save_path + f\"saved_model_{datetime.now().strftime('%Y%m%d_%H%M')}\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. ä¿å­˜è®­ç»ƒå†å²å’Œé…ç½®\n",
    "\"\"\"\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒé…ç½®\n",
    "config = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'model_architecture': 'ResNet50'\n",
    "}\n",
    "with open(model_save_path + 'config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒå†å²\n",
    "with open(model_save_path + 'training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"æ¨¡å‹ä¿å­˜ç›®å½•å·²åˆ›å»º: {model_save_path}\")\n",
    "print(\"è¯·åœ¨è®­ç»ƒå®Œæˆåå–æ¶ˆæ³¨é‡Šç›¸åº”ä»£ç æ¥ä¿å­˜æ¨¡å‹\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 æ¨¡å‹åŠ è½½ä¸æ–­ç‚¹ç»­è®­\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹åŠ è½½å’Œæ–­ç‚¹ç»­è®­\n",
    "import glob\n",
    "\n",
    "# 1. ä»äº‘ç›˜åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "def load_latest_model(model_dir, model_type=\"pytorch\"):\n",
    "    \"\"\"åŠ è½½æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶\"\"\"\n",
    "    if model_type == \"pytorch\":\n",
    "        model_files = glob.glob(os.path.join(model_dir, \"*.pth\"))\n",
    "    else:  # tensorflow\n",
    "        model_files = glob.glob(os.path.join(model_dir, \"*.h5\"))\n",
    "    \n",
    "    if model_files:\n",
    "        latest_model = max(model_files, key=os.path.getctime)\n",
    "        print(f\"æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {latest_model}\")\n",
    "        return latest_model\n",
    "    else:\n",
    "        print(\"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶\")\n",
    "        return None\n",
    "\n",
    "# 2. PyTorchæ¨¡å‹åŠ è½½ç¤ºä¾‹\n",
    "\"\"\"\n",
    "# åŠ è½½æ¨¡å‹æƒé‡\n",
    "model = YourModel()  # å…ˆåˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "latest_model_path = load_latest_model(model_save_path, \"pytorch\")\n",
    "if latest_model_path:\n",
    "    model.load_state_dict(torch.load(latest_model_path))\n",
    "    print(\"PyTorchæ¨¡å‹æƒé‡åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# åŠ è½½å®Œæ•´æ¨¡å‹\n",
    "# model = torch.load(latest_model_path)\n",
    "\"\"\"\n",
    "\n",
    "# 3. TensorFlowæ¨¡å‹åŠ è½½ç¤ºä¾‹\n",
    "\"\"\"\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "latest_model_path = load_latest_model(model_save_path, \"tensorflow\") \n",
    "if latest_model_path:\n",
    "    model = load_model(latest_model_path)\n",
    "    print(\"TensorFlowæ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\"\"\"\n",
    "\n",
    "# 4. æ–­ç‚¹ç»­è®­è®¾ç½®\n",
    "\"\"\"\n",
    "# è®¾ç½®æ£€æŸ¥ç‚¹å›è°ƒ\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_path = model_save_path + \"checkpoint_epoch_{epoch:02d}.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch',  # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# è®­ç»ƒæ—¶ä½¿ç”¨å›è°ƒ\n",
    "# model.fit(X_train, y_train, callbacks=[checkpoint_callback])\n",
    "\"\"\"\n",
    "\n",
    "print(\"æ¨¡å‹åŠ è½½å’Œæ–­ç‚¹ç»­è®­ä»£ç æ¨¡æ¿å·²å‡†å¤‡å¥½\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å®ç”¨æŠ€å·§\n",
    "\n",
    "### 4.1 é˜²æ­¢è¿æ¥è¶…æ—¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colabå®ç”¨æŠ€å·§åˆé›†\n",
    "\n",
    "# 1. é˜²æ­¢è¿æ¥è¶…æ—¶çš„JavaScriptä»£ç \n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "def prevent_colab_timeout():\n",
    "    \"\"\"é˜²æ­¢Colabå› ä¸ºä¸æ´»è·ƒè€Œæ–­å¼€è¿æ¥\"\"\"\n",
    "    display(Javascript('''\n",
    "    function ClickConnect(){\n",
    "        console.log(\"Working\");\n",
    "        document.querySelector(\"colab-connect-button\").click()\n",
    "    }\n",
    "    setInterval(ClickConnect,60000)\n",
    "    '''))\n",
    "\n",
    "# è¿è¡Œæ­¤å‡½æ•°æ¥é˜²æ­¢è¶…æ—¶\n",
    "# prevent_colab_timeout()\n",
    "\n",
    "# 2. æ¸…ç†GPUå†…å­˜\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"æ¸…ç†GPUå†…å­˜\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"PyTorch GPUå†…å­˜å·²æ¸…ç†\")\n",
    "    \n",
    "    # TensorFlowæ¸…ç†\n",
    "    try:\n",
    "        from tensorflow.keras import backend as K\n",
    "        K.clear_session()\n",
    "        print(\"TensorFlowå†…å­˜å·²æ¸…ç†\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 3. ç›‘æ§è®­ç»ƒè¿›åº¦\n",
    "def setup_training_monitor():\n",
    "    \"\"\"è®¾ç½®è®­ç»ƒç›‘æ§\"\"\"\n",
    "    code = \"\"\"\n",
    "    # å¯ä»¥åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨\n",
    "    from tqdm import tqdm\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # è¿›åº¦æ¡\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "        # è®­ç»ƒä»£ç \n",
    "        pass\n",
    "    \n",
    "    # å®æ—¶ç»˜å›¾\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    def plot_training_progress(train_losses, val_losses):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training Progress')\n",
    "        plt.show()\n",
    "    \"\"\"\n",
    "    print(\"è®­ç»ƒç›‘æ§ä»£ç æ¨¡æ¿:\")\n",
    "    print(code)\n",
    "\n",
    "setup_training_monitor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å¸¸ç”¨å‘½ä»¤å’Œå¿«æ·é”®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¸ç”¨æŠ€å·§æ€»ç»“\n",
    "\n",
    "print(\"=== Colab å®ç”¨æŠ€å·§æ€»ç»“ ===\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Œ é‡è¦æç¤º:\")\n",
    "print(\"1. å…è´¹ç‰ˆGPUä½¿ç”¨æ—¶é—´æœ‰é™(çº¦12å°æ—¶)ï¼Œåˆç†å®‰æ’è®­ç»ƒæ—¶é—´\")\n",
    "print(\"2. è¿è¡Œæ—¶ä¼šåœ¨12å°æ—¶åæˆ–ä¸æ´»è·ƒ90åˆ†é’Ÿåé‡ç½®\")\n",
    "print(\"3. é‡è¦æ•°æ®åŠ¡å¿…ä¿å­˜åˆ°äº‘ç›˜ï¼\")\n",
    "print()\n",
    "\n",
    "print(\"âŒ¨ï¸ å¸¸ç”¨å¿«æ·é”®:\")\n",
    "print(\"- Ctrl+Enter: è¿è¡Œå½“å‰å•å…ƒæ ¼\")\n",
    "print(\"- Shift+Enter: è¿è¡Œå½“å‰å•å…ƒæ ¼å¹¶é€‰æ‹©ä¸‹ä¸€ä¸ª\")\n",
    "print(\"- Ctrl+M+A: åœ¨ä¸Šæ–¹æ’å…¥å•å…ƒæ ¼\")\n",
    "print(\"- Ctrl+M+B: åœ¨ä¸‹æ–¹æ’å…¥å•å…ƒæ ¼\")\n",
    "print(\"- Ctrl+M+D: åˆ é™¤å•å…ƒæ ¼\")\n",
    "print(\"- Ctrl+M+Y: åˆ‡æ¢ä¸ºä»£ç å•å…ƒæ ¼\")\n",
    "print(\"- Ctrl+M+M: åˆ‡æ¢ä¸ºMarkdownå•å…ƒæ ¼\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ æœ€ä½³å®è·µ:\")\n",
    "print(\"- å®šæœŸä¿å­˜checkpointsåˆ°äº‘ç›˜\")\n",
    "print(\"- ä½¿ç”¨ç›¸å¯¹è·¯å¾„å¼•ç”¨äº‘ç›˜æ–‡ä»¶\")\n",
    "print(\"- è®­ç»ƒå‰æ£€æŸ¥GPUå¯ç”¨æ€§\")\n",
    "print(\"- é•¿æ—¶é—´è®­ç»ƒå»ºè®®å¼€å¯é˜²æ–­çº¿åŠŸèƒ½\")\n",
    "print(\"- åŠæ—¶æ¸…ç†ä¸éœ€è¦çš„æ–‡ä»¶é‡Šæ”¾ç©ºé—´\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”§ è°ƒè¯•æŠ€å·§:\")\n",
    "print(\"- ä½¿ç”¨ !nvidia-smi æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ\")\n",
    "print(\"- ä½¿ç”¨ !df -h æŸ¥çœ‹ç£ç›˜ç©ºé—´\")\n",
    "print(\"- ä½¿ç”¨ !ps aux | grep python æŸ¥çœ‹è¿è¡Œè¿›ç¨‹\")\n",
    "print(\"- è®­ç»ƒå¡ä½æ—¶é‡å¯è¿è¡Œæ—¶: è¿è¡Œæ—¶ -> é‡å¯è¿è¡Œæ—¶\")\n",
    "\n",
    "# å¿«é€Ÿè®¾ç½®ä»£ç æ¨¡æ¿\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸš€ å¿«é€Ÿå¼€å§‹æ¨¡æ¿:\")\n",
    "print(\"1. æŒ‚è½½äº‘ç›˜: drive.mount('/content/drive')\")\n",
    "print(\"2. æ£€æŸ¥GPU: torch.cuda.is_available()\")\n",
    "print(\"3. è®¾ç½®è·¯å¾„: data_path = '/content/drive/MyDrive/...'\")\n",
    "print(\"4. å¼€å§‹è®­ç»ƒå¹¶å®šæœŸä¿å­˜åˆ°äº‘ç›˜\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNSmRC6XIo88nWEk/FmQpdz",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
